"""Agent coordination benchmark demonstration.

Demonstrates multi-agent coordination with temporal optimization for
complex collaborative tasks requiring synchronized AI interactions.
"""

import asyncio
import time
import json
from typing import Dict, List, Any, Optional
import sys
import os
from dataclasses import dataclass

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from tboptimizer import (
    ClaudeOptimizedClient,
    OptimizationEngine,
    CollaborationContext,
    OptimizationLevel,
    TimingPlan
)


@dataclass
class Agent:
    """Represents an AI agent in the coordination system."""
    
    id: str
    role: str
    capabilities: List[str]
    client: ClaudeOptimizedClient
    estimated_duration: float = 2.0
    dependencies: List[str] = None
    
    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []


class AgentCoordinationBenchmark:
    """Benchmark for multi-agent coordination with temporal optimization."""
    
    def __init__(self, api_key: Optional[str] = None):
        """Initialize coordination benchmark."""
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.optimization_engine = OptimizationEngine(
            optimization_level=OptimizationLevel.RESEARCH,
            target_latency=0.5
        )
        
        self.agents: List[Agent] = []
        self.coordination_results = {}
    
    def setup_agent_team(self) -> List[Agent]:
        """Setup a team of specialized AI agents."""
        
        agent_configs = [
            {
                "id": "analyst",
                "role": "Data Analyst",
                "capabilities": ["data_analysis", "pattern_recognition", "statistical_inference"],
                "estimated_duration": 2.5,
                "dependencies": []
            },
            {
                "id": "strategist", 
                "role": "Strategic Planner",
                "capabilities": ["strategic_thinking", "planning", "risk_assessment"],
                "estimated_duration": 3.0,
                "dependencies": ["analyst"]
            },
            {
                "id": "implementer",
                "role": "Implementation Specialist", 
                "capabilities": ["technical_design", "implementation_planning", "optimization"],
                "estimated_duration": 2.2,
                "dependencies": ["strategist"]
            },
            {
                "id": "reviewer",
                "role": "Quality Reviewer",
                "capabilities": ["quality_assurance", "validation", "improvement_recommendations"],
                "estimated_duration": 1.8,
                "dependencies": ["analyst", "strategist", "implementer"]
            }
        ]
        
        agents = []
        for config in agent_configs:
            # Create optimized client for each agent
            client = ClaudeOptimizedClient(
                api_key=self.api_key,
                optimization_level=OptimizationLevel.RESEARCH,
                enable_monitoring=True,
                target_latency=0.4  # Aggressive latency target for coordination
            )
            
            agent = Agent(
                id=config["id"],
                role=config["role"],
                capabilities=config["capabilities"],
                client=client,
                estimated_duration=config["estimated_duration"],
                dependencies=config["dependencies"]
            )
            
            agents.append(agent)
        
        self.agents = agents
        return agents
    
    async def run_coordination_benchmark(self) -> Dict[str, Any]:
        """Run multi-agent coordination benchmark."""
        
        print("ü§ñ Multi-Agent Coordination Benchmark")
        print("=" * 45)
        print("Testing temporal optimization in coordinated AI collaboration")
        print()
        
        # Setup agent team
        agents = self.setup_agent_team()
        print(f"Initialized {len(agents)} specialized agents:")
        for agent in agents:
            print(f"  - {agent.role} ({agent.id})")
        print()
        
        # Test different coordination strategies
        strategies = ["sequential", "parallel", "adaptive"]
        
        benchmark_results = {}
        
        for strategy in strategies:
            print(f"üìä Testing {strategy} coordination strategy...")
            
            strategy_results = await self._run_coordination_strategy(agents, strategy)
            benchmark_results[strategy] = strategy_results
            
            print(f"   Completed in {strategy_results['total_time']:.1f}s")
            print(f"   Efficiency: {strategy_results['coordination_efficiency']:.3f}")
            print()
        
        # Analyze results
        analysis = self._analyze_coordination_results(benchmark_results)
        self._display_coordination_results(analysis)
        
        return {
            "benchmark_results": benchmark_results,
            "analysis": analysis,
            "agents": [{"id": a.id, "role": a.role, "capabilities": a.capabilities} for a in agents]
        }
    
    async def _run_coordination_strategy(self, 
                                       agents: List[Agent], 
                                       strategy: str) -> Dict[str, Any]:
        """Run specific coordination strategy."""
        
        # Create timing plan
        agent_configs = [
            {
                "id": agent.id,
                "estimated_duration": agent.estimated_duration,
                "dependencies": agent.dependencies
            }
            for agent in agents
        ]
        
        timing_plan = self.optimization_engine.coordinate_multi_agent_timing(
            agents=agent_configs,
            coordination_strategy=strategy
        )
        
        # Complex collaborative task
        task_context = {
            "task": "market_analysis",
            "description": "Comprehensive market analysis for new product launch",
            "requirements": [
                "Analyze market data and trends",
                "Develop strategic recommendations", 
                "Create implementation roadmap",
                "Review and validate approach"
            ]
        }
        
        start_time = time.time()
        agent_results = {}
        
        if strategy == "parallel":
            # Execute agents in parallel where possible
            agent_results = await self._execute_parallel_coordination(agents, task_context, timing_plan)
        
        elif strategy == "sequential":
            # Execute agents sequentially 
            agent_results = await self._execute_sequential_coordination(agents, task_context, timing_plan)
        
        else:  # adaptive
            # Adaptive coordination based on dependencies
            agent_results = await self._execute_adaptive_coordination(agents, task_context, timing_plan)
        
        total_time = time.time() - start_time
        
        # Calculate coordination efficiency
        total_productive_time = sum(r.get("duration", 0) for r in agent_results.values())
        coordination_efficiency = total_productive_time / total_time if total_time > 0 else 0
        
        return {
            "strategy": strategy,
            "total_time": total_time,
            "productive_time": total_productive_time,
            "coordination_efficiency": coordination_efficiency,
            "agent_results": agent_results,
            "timing_plan": {
                "agent_schedules": timing_plan.agent_schedules,
                "total_duration": timing_plan.total_duration,
                "parallel_execution": timing_plan.parallel_execution
            }
        }
    
    async def _execute_parallel_coordination(self, 
                                           agents: List[Agent],
                                           task_context: Dict[str, Any],
                                           timing_plan: TimingPlan) -> Dict[str, Any]:
        """Execute agents in parallel coordination."""
        
        # Create tasks for all agents simultaneously
        agent_tasks = []
        
        for agent in agents:
            task_prompt = self._generate_agent_prompt(agent, task_context)
            
            context = CollaborationContext(
                session_id=f"coordination_{agent.id}_{int(time.time())}",
                task_type="market_analysis",
                complexity=4,
                priority=2,
                metadata={"coordination_strategy": "parallel", "agent_role": agent.role}
            )
            
            # Create async task for this agent
            agent_task = asyncio.create_task(
                self._execute_agent_task(agent, task_prompt, context)
            )
            agent_tasks.append((agent.id, agent_task))
        
        # Wait for all agents to complete
        results = {}
        for agent_id, task in agent_tasks:
            try:
                results[agent_id] = await task
            except Exception as e:
                results[agent_id] = {"error": str(e), "duration": 0}
        
        return results
    
    async def _execute_sequential_coordination(self,
                                             agents: List[Agent],
                                             task_context: Dict[str, Any], 
                                             timing_plan: TimingPlan) -> Dict[str, Any]:
        """Execute agents in sequential coordination."""
        
        results = {}
        
        for agent in agents:
            task_prompt = self._generate_agent_prompt(agent, task_context, previous_results=results)
            
            context = CollaborationContext(
                session_id=f"coordination_{agent.id}_{int(time.time())}",
                task_type="market_analysis",
                complexity=4,
                priority=2,
                metadata={"coordination_strategy": "sequential", "agent_role": agent.role}
            )
            
            try:
                result = await self._execute_agent_task(agent, task_prompt, context)
                results[agent.id] = result
            except Exception as e:
                results[agent.id] = {"error": str(e), "duration": 0}
        
        return results
    
    async def _execute_adaptive_coordination(self,
                                           agents: List[Agent],
                                           task_context: Dict[str, Any],
                                           timing_plan: TimingPlan) -> Dict[str, Any]:
        """Execute agents using adaptive coordination based on dependencies."""
        
        results = {}
        remaining_agents = agents.copy()
        
        while remaining_agents:
            # Find agents with satisfied dependencies
            ready_agents = []
            for agent in remaining_agents:
                if all(dep in results for dep in agent.dependencies):
                    ready_agents.append(agent)
            
            if not ready_agents:
                # Break circular dependencies
                ready_agents = [remaining_agents[0]]
            
            # Execute ready agents in parallel
            agent_tasks = []
            for agent in ready_agents:
                task_prompt = self._generate_agent_prompt(agent, task_context, previous_results=results)
                
                context = CollaborationContext(
                    session_id=f"coordination_{agent.id}_{int(time.time())}",
                    task_type="market_analysis",
                    complexity=4,
                    priority=2,
                    metadata={"coordination_strategy": "adaptive", "agent_role": agent.role}
                )
                
                agent_task = asyncio.create_task(
                    self._execute_agent_task(agent, task_prompt, context)
                )
                agent_tasks.append((agent.id, agent_task))
            
            # Wait for batch completion
            for agent_id, task in agent_tasks:
                try:
                    results[agent_id] = await task
                except Exception as e:
                    results[agent_id] = {"error": str(e), "duration": 0}
            
            # Remove completed agents
            for agent in ready_agents:
                remaining_agents.remove(agent)
        
        return results
    
    async def _execute_agent_task(self, 
                                agent: Agent,
                                prompt: str,
                                context: CollaborationContext) -> Dict[str, Any]:
        """Execute a single agent task."""
        
        start_time = time.time()
        
        try:
            messages = [{"role": "user", "content": prompt}]
            
            response = await agent.client.collaborate(
                messages=messages,
                context=context,
                max_tokens=300
            )
            
            duration = time.time() - start_time
            
            return {
                "agent_id": agent.id,
                "role": agent.role,
                "duration": duration,
                "response": response.content,
                "optimization_applied": response.optimization_applied,
                "cache_hit": response.cache_hit,
                "response_time": response.response_time
            }
            
        except Exception as e:
            duration = time.time() - start_time
            # Fallback simulation for demo
            await asyncio.sleep(agent.estimated_duration * 0.5)  # Simulate partial work
            
            return {
                "agent_id": agent.id,
                "role": agent.role,
                "duration": duration + agent.estimated_duration * 0.5,
                "response": f"Simulated {agent.role} analysis completed",
                "optimization_applied": False,
                "cache_hit": False,
                "error": str(e)
            }
    
    def _generate_agent_prompt(self, 
                             agent: Agent,
                             task_context: Dict[str, Any],
                             previous_results: Dict[str, Any] = None) -> str:
        """Generate specialized prompt for each agent."""
        
        base_task = task_context["description"]
        
        role_prompts = {
            "analyst": f"""As a Data Analyst, analyze market data for the new product launch.
Focus on market trends, customer segments, and competitive landscape.
Provide data-driven insights and key metrics.

Task: {base_task}""",
            
            "strategist": f"""As a Strategic Planner, develop strategic recommendations based on the analysis.
Create a comprehensive strategy addressing market positioning and competitive advantages.

Task: {base_task}""",
            
            "implementer": f"""As an Implementation Specialist, create a detailed implementation roadmap.
Focus on practical steps, resource requirements, and timeline considerations.

Task: {base_task}""",
            
            "reviewer": f"""As a Quality Reviewer, review and validate the overall approach.
Identify potential risks, gaps, and improvement opportunities.

Task: {base_task}"""
        }
        
        prompt = role_prompts.get(agent.id, f"Complete this task: {base_task}")
        
        # Add context from previous agents if available
        if previous_results:
            prompt += "\n\nPrevious analysis from team members:\n"
            for agent_id, result in previous_results.items():
                if "response" in result:
                    prompt += f"- {agent_id}: {result['response'][:100]}...\n"
        
        return prompt
    
    def _analyze_coordination_results(self, benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze coordination benchmark results."""
        
        analysis = {
            "strategy_comparison": {},
            "efficiency_metrics": {},
            "optimization_impact": {}
        }
        
        # Compare strategies
        for strategy, results in benchmark_results.items():
            analysis["strategy_comparison"][strategy] = {
                "total_time": results["total_time"],
                "coordination_efficiency": results["coordination_efficiency"],
                "parallel_execution": results["timing_plan"]["parallel_execution"]
            }
        
        # Find best strategy
        best_strategy = min(benchmark_results.keys(), 
                          key=lambda s: benchmark_results[s]["total_time"])
        worst_strategy = max(benchmark_results.keys(),
                           key=lambda s: benchmark_results[s]["total_time"])
        
        best_time = benchmark_results[best_strategy]["total_time"]
        worst_time = benchmark_results[worst_strategy]["total_time"]
        
        analysis["efficiency_metrics"] = {
            "best_strategy": best_strategy,
            "worst_strategy": worst_strategy,
            "time_improvement": ((worst_time - best_time) / worst_time) * 100,
            "coordination_ranking": sorted(
                benchmark_results.keys(),
                key=lambda s: benchmark_results[s]["coordination_efficiency"],
                reverse=True
            )
        }
        
        # Analyze optimization impact
        total_cache_hits = 0
        total_requests = 0
        
        for strategy, results in benchmark_results.items():
            for agent_id, agent_result in results["agent_results"].items():
                if "cache_hit" in agent_result:
                    total_requests += 1
                    if agent_result["cache_hit"]:
                        total_cache_hits += 1
        
        analysis["optimization_impact"] = {
            "cache_hit_rate": total_cache_hits / total_requests if total_requests > 0 else 0,
            "total_coordinated_requests": total_requests,
            "optimization_effectiveness": "high" if total_cache_hits / total_requests > 0.3 else "moderate"
        }
        
        return analysis
    
    def _display_coordination_results(self, analysis: Dict[str, Any]):
        """Display coordination benchmark results."""
        
        print("üéØ COORDINATION BENCHMARK RESULTS")
        print("=" * 40)
        print()
        
        # Strategy comparison
        print("Strategy Performance:")
        for strategy, metrics in analysis["strategy_comparison"].items():
            print(f"  {strategy.title()}:")
            print(f"    Total time: {metrics['total_time']:.1f}s")
            print(f"    Efficiency: {metrics['coordination_efficiency']:.3f}")
            print(f"    Parallel:   {metrics['parallel_execution']}")
            print()
        
        # Best strategy
        efficiency_metrics = analysis["efficiency_metrics"]
        print(f"üèÜ Best Strategy: {efficiency_metrics['best_strategy'].title()}")
        print(f"   Time improvement: {efficiency_metrics['time_improvement']:.1f}% faster")
        print()
        
        print("Efficiency Ranking:")
        for i, strategy in enumerate(efficiency_metrics["coordination_ranking"], 1):
            print(f"  {i}. {strategy.title()}")
        print()
        
        # Optimization impact
        opt_impact = analysis["optimization_impact"]
        print("Optimization Impact:")
        print(f"  Cache hit rate: {opt_impact['cache_hit_rate']:.1%}")
        print(f"  Total requests: {opt_impact['total_coordinated_requests']}")
        print(f"  Effectiveness:  {opt_impact['optimization_effectiveness']}")
        print()
        
        # Recommendations
        print("üí° Recommendations:")
        best_strategy = efficiency_metrics['best_strategy']
        
        if best_strategy == "adaptive":
            print("   - Adaptive coordination provides optimal performance")
            print("   - Consider dependency-aware scheduling for complex workflows")
        elif best_strategy == "parallel":
            print("   - Parallel execution maximizes throughput")
            print("   - Suitable for independent agent tasks")
        else:
            print("   - Sequential coordination ensures proper dependencies")
            print("   - Recommended for tightly coupled workflows")


async def run_benchmark():
    """Run the agent coordination benchmark."""
    
    benchmark = AgentCoordinationBenchmark()
    
    try:
        results = await benchmark.run_coordination_benchmark()
        
        print("\n" + "=" * 50)
        print("Agent coordination benchmark completed!")
        print("Results demonstrate temporal optimization in multi-agent scenarios.")
        
        return results
        
    except Exception as e:
        print(f"\n‚ùå Benchmark failed: {e}")
        print("This may be due to API configuration issues.")
        return {"error": str(e)}


if __name__ == "__main__":
    print("Starting multi-agent coordination benchmark...")
    print("This demonstrates temporal optimization in coordinated AI collaboration.")
    print()
    
    # Run the benchmark
    results = asyncio.run(run_benchmark())
    
    if results and "error" not in results:
        print("\nüéØ Next Steps:")
        print("1. Try production deployment (production_deployment_example.py)")
        print("2. Run full validation study (research/validation_study.py)")
        print("3. Integrate with existing AI workflows")
    
    print("\nüìñ For implementation details, see the documentation in docs/")